---
title: Abstracts
nav: true
---

# Abstracts

## Claudio Muñoz. Bounds on the approximation error for deep neural networks applied to dispersive models: Nonlinear waves
<p style="text-align: justify;">
We present a framework for deriving rigorous and efficient bounds on the approximation error of deep neural networks in PDE models characterized by branching mechanisms, such as waves, Schrödinger equations, and other dispersive models. This framework utilizes the probabilistic setting established by Henry-Labordère and Touzi. We illustrate this approach by providing rigorous bounds on the approximation error for both linear and nonlinear waves in physical dimensions d=1,2,3, and analyze their respective computational costs starting from time zero. We investigate two key scenarios: one involving a linear perturbative source term, and another focusing on pure nonlinear internal interactions. This is joint work with Nicolás Valenzuela.
</p>

## Mircea Petrache. A Communication Framework for Compositional Generation 
<p style="text-align: justify;">
Compositionality and compositional generalization -- the ability to understand novel combinations of known concepts --  are central characteristics of human language and are hypothesized to be essential for human cognition. In machine learning, the emergence of this property has been studied in a communication game setting, where independent agents (a sender and a receiver) converge to a shared encoding policy from a set of states to a space of discrete messages, where the receiver can correctly reconstruct the states observed by the sender using only the sender's messages. The use of communication games in data generation tasks is still largely unexplored, with recent methods for compositional generation focusing mainly on the use of supervised guidance (either through class labels or text). 
I will present recent work https://arxiv.org/abs/2501.19182, (part of the master thesis work of Rafael Elberg, whom I co-supervise with Denis Parra), an approach in which we take the first steps to fill the above gap, and we present a self-supervised generative communication game-based framework for creating compositional encodings in learned representations from pre-trained encoder-decoder models. In an Iterated Learning (IL) protocol involving a sender and a receiver, we apply alternating pressures for compression and diversity of encoded discrete messages, so that the protocol converges to an efficient but unambiguous encoding.
</p>

